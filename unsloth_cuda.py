import os
os.environ["UNSLOTH_PATCH_RL_TRAINERS"] = "false"
os.environ["UNSLOTH_COMPILE_DISABLE"] = "1"
os.environ["TORCHINDUCTOR_DISABLED"] = "1"

import json
import shutil
import subprocess
from pathlib import Path

from datasets import Dataset                             # TODO
from unsloth import FastLanguageModel                    # TODO
from trl import SFTTrainer, SFTConfig                    # TODO

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Constants
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_DIR      = Path("./model")
LORA_OUTPUT    = Path("./lora")
MERGED_DIR     = Path("./merged")
GGUF_DIR       = Path("./gguf")
GGUF_PATH      = GGUF_DIR / "model.gguf"

GGUF_CONVERTER = Path(r"C:\ExampleProjects\llama.cpp\convert_hf_to_gguf.py")
PYTHON_BIN     = Path(".venv/Scripts/python.exe").resolve()

DATA_PATH      = Path("gemma_lora_data.json")
MAX_SEQ_LEN    = 4096
NUM_EPOCHS     = 3
BATCH_SIZE     = 2
LR             = 2e-4

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("ğŸ§¹[1/7] Cleaning previous artefactsâ€¦")
    for _dir in (LORA_OUTPUT, MERGED_DIR, GGUF_DIR):
        if _dir.exists():
            shutil.rmtree(_dir)
            print(f"  â€‘ removed Â«{_dir}Â»")
    print("  âœ”ï¸Finished cleaning")

    print("ğŸ“„[2/7] Reading training examplesâ€¦")
    with DATA_PATH.open(encoding="utf-8") as fp:
        records = json.load(fp)

    def build_chat(example):  # TODO
        prompt = example["prompt"].strip()
        response = example["response"].strip()
        return {
            "text": (
                f"<start_of_turn>user\n{prompt}<end_of_turn>\n"
                f"<start_of_turn>model\n{response}<end_of_turn>\n"
            )
        }

    ds = Dataset.from_list([build_chat(r) for r in records])
    print(f"  âœ”ï¸Loaded {len(ds):,} samples")

    print(f"ğŸ¦¥[3/7] Loading base model from Â«{MODEL_DIR}Â» â€¦")
    model, tokenizer = FastLanguageModel.from_pretrained(  # TODO
        model_name=str(MODEL_DIR),
        max_seq_length=MAX_SEQ_LEN,
        load_in_4bit=True,
        dtype=None,
    )

    model = FastLanguageModel.get_peft_model(model)  # attach LoRA
    print("  âœ”ï¸Model ready for fineâ€‘tuning")

    print("ğŸ¯[4/7] Starting supervised fineâ€‘tuning â€¦")
    sft_cfg = SFTConfig(
        per_device_train_batch_size=BATCH_SIZE,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LR,
        logging_steps=10,
        dataset_num_proc=1,
    )

    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=ds,
        args=sft_cfg,
        dataset_text_field="text"
    )

    trainer.train()
    print("  âœ”ï¸Training complete")

    print(f"ğŸ’¾[5/7] Saving LoRA weights to Â«{LORA_OUTPUT}Â» â€¦")
    LORA_OUTPUT.mkdir(parents=True, exist_ok=True)
    model.save_pretrained_merged(str(LORA_OUTPUT), tokenizer, save_method="lora")
    print("  âœ”ï¸Adapters saved")

    print("ğŸ”—[6/7] Merging LoRA + base âŸ¶ fp16 â€¦")
    MERGED_DIR.mkdir(parents=True, exist_ok=True)
    model.save_pretrained_merged(str(MERGED_DIR), tokenizer, save_method="merged_16bit")
    print(f"  âœ”ï¸Merged model written to Â«{MERGED_DIR}Â»")

    print("ğŸ”„[7/7] Converting to GGUF (q4_K_M) â€¦")
    GGUF_DIR.mkdir(parents=True, exist_ok=True)
    subprocess.run(
        [
            str(PYTHON_BIN),
            str(GGUF_CONVERTER),
            os.path.abspath(MERGED_DIR),
            "--outfile", str(GGUF_PATH),
            "--outtype", "q8_0",
        ],
        check=True,
    )
    print(f"  âœ”ï¸GGUF ready at Â«{GGUF_PATH}Â»")
    print("ğŸPipeline finished successfully!")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Required for Windows multiprocessing
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import multiprocessing
    multiprocessing.freeze_support()
    main()
